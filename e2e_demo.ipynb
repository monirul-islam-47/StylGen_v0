{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f706f356",
   "metadata": {},
   "source": [
    "# Step 1 — start the server (uses HashingEmbedder; kills any prior server in this kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d34de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server PID: 18355 BASE: http://127.0.0.1:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 16:20:28,089 INFO [stylgen] embedder.selected kind=hash dim=384\n",
      "INFO:     Started server process [18355]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: start FastAPI server (background)\n",
    "import os, sys, time, subprocess\n",
    "  \n",
    "HOST, PORT = \"127.0.0.1\", 8000\n",
    "BASE = f\"http://{HOST}:{PORT}\"\n",
    "  \n",
    "# Stop previous server in this kernel if running\n",
    "try:\n",
    "    SERVER_PROC\n",
    "except NameError:\n",
    "      SERVER_PROC = None\n",
    "  \n",
    "if SERVER_PROC and SERVER_PROC.poll() is None:\n",
    "      SERVER_PROC.terminate()\n",
    "      try:\n",
    "          SERVER_PROC.wait(timeout=5)\n",
    "      except Exception:\n",
    "          SERVER_PROC.kill()\n",
    "  \n",
    "env = os.environ.copy()\n",
    "env[\"STYLGEN_LOG_LEVEL\"] = \"INFO\"   # or \"DEBUG\"\n",
    "env[\"STYLGEN_DEBUG\"] = \"0\"\n",
    "env[\"STYLGEN_EMBEDDER\"] = \"hash\"    # we'll switch to ST later\n",
    "env[\"OLLAMA_BASE\"] = \"http://127.0.0.1:11434\"\n",
    "env[\"OLLAMA_MODEL\"] = \"llama3:8b\"\n",
    "  \n",
    "cmd = [\n",
    "      sys.executable, \"-m\", \"uvicorn\", \"stylgen_v0.main:app\",\n",
    "      \"--host\", HOST, \"--port\", str(PORT), \"--log-level\", \"info\"\n",
    "  ]\n",
    "SERVER_PROC = subprocess.Popen(cmd, env=env)\n",
    "print(\"Server PID:\", SERVER_PROC.pid, \"BASE:\", BASE)\n",
    "time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11877637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env[\"OLLAMA_BASE\"] = \"http://127.0.0.1:11434\"\n",
    "# env[\"OLLAMA_MODEL\"] = \"llama3:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41099c0d",
   "metadata": {},
   "source": [
    "# Step 2 — health check (waits briefly for the server to come up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752deb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:48590 - \"GET /health HTTP/1.1\" 200 OK\n",
      "Health: 200 {'status': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: health check\n",
    "import time, httpx\n",
    "  \n",
    "for _ in range(40):  # ~10s total\n",
    "      try:\n",
    "          r = httpx.get(f\"{BASE}/health\", timeout=2.0)\n",
    "          print(\"Health:\", r.status_code, r.json())\n",
    "          break\n",
    "      except Exception:\n",
    "          time.sleep(0.25)\n",
    "else:\n",
    "      raise RuntimeError(\"Server did not become healthy in time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7090c2",
   "metadata": {},
   "source": [
    "# Step 3 — create a persona (example samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30dbba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59642 - \"POST /persona HTTP/1.1\" 200 OK\n",
      "✓ Persona created\n",
      "user_id: u_nb1 num_samples: 3\n",
      "exemplar_ids: ['5794076f-f47a-4b51-bb89-c36adc0893d8', 'd1894100-8da2-4282-9778-755e434e95ac', '472ddc45-bc60-4faa-bb63-7e3fae80fa84']\n",
      "centroid_dim: 384\n",
      "tone: ['forthright', 'practical', 'lightly humorous']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 16:24:29,736 INFO [stylgen] persona.create user_id=u_nb1 samples=3\n",
      "2025-09-06 16:24:29,737 INFO [stylgen] persona.created user_id=u_nb1 exemplars=3 centroid=yes\n"
     ]
    }
   ],
   "source": [
    "# Step 3: create persona (example)\n",
    "import httpx, textwrap, json\n",
    "  \n",
    "user_id = \"u_nb1\"  # change if you like\n",
    "  \n",
    "persona_payload = {\n",
    "      \"user_id\": user_id,\n",
    "      \"samples\": [\n",
    "          \"Shipped our onboarding revamp. Short, friendly checklists beat long docs.\",\n",
    "          \"If your standup drags, it's a smell. Keep it under 10 minutes, tops.\",\n",
    "          \"Docs are a product. If you don't version them, they'll version you.\"\n",
    "      ],\n",
    "      \"preferences\": {\n",
    "          \"tone_descriptors\": [\"forthright\", \"practical\", \"lightly humorous\"],\n",
    "          \"taboo_phrases\": [\"In today's fast-paced world\"],\n",
    "          \"formality\": 2,\n",
    "          \"emoji_ok\": True,\n",
    "          \"hashtags_niche\": True,\n",
    "          \"structure_pref\": \"story-first\"\n",
    "      }\n",
    "  }\n",
    "  \n",
    "r = httpx.post(f\"{BASE}/persona\", json=persona_payload, timeout=15.0)\n",
    "r.raise_for_status()\n",
    "resp = r.json()\n",
    "card = resp[\"persona\"]\n",
    "  \n",
    "print(\"✓ Persona created\")\n",
    "print(\"user_id:\", resp[\"user_id\"], \"num_samples:\", resp[\"num_samples\"])\n",
    "print(\"exemplar_ids:\", card[\"exemplar_ids\"])\n",
    "print(\"centroid_dim:\", None if card[\"centroid\"] is None else len(card[\"centroid\"]))\n",
    "print(\"tone:\", card[\"preferences\"][\"tone_descriptors\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecc221",
   "metadata": {},
   "source": [
    "# Step 4 — generate 2 variants and preview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4184e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 16:33:38,968 INFO [stylgen] generate.request user_id=u_nb1 goal=educate keywords=onboarding,dev teams variants=2 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:33284 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "✓ Generated 2 variants (generation_id=060bfecc-ea30-409f-94d5-e606430ed064)\n",
      "Chosen preview: Here's a draft for a LinkedIn post: / / \"📊 As an engineering manager, I've seen my fair share of onboarding disasters. But after our recent revamp, I'm [...]\n",
      "Scores:\n",
      "  v0: style=0.330 nov=0.763 length_ok=True\n",
      "  v1: style=0.335 nov=0.707 length_ok=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 16:33:44,299 INFO [stylgen.pipeline] variants.sorted count=2 top_sim=0.330 top_nov=0.763 length_ok=True\n",
      "2025-09-06 16:33:44,299 INFO [stylgen] generate.done user_id=u_nb1 generation_id=060bfecc-ea30-409f-94d5-e606430ed064 chosen_sim=0.330 chosen_novelty=0.763\n"
     ]
    }
   ],
   "source": [
    "# Step 4: generate 2 variants (non-stream), quick preview\n",
    "import httpx, textwrap\n",
    "  \n",
    "gen_payload = {\n",
    "      \"user_id\": user_id,  # from Step 3\n",
    "      \"brief\": {\n",
    "          \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "          \"goal\": \"educate\",\n",
    "          \"audience\": \"engineering managers\",\n",
    "          \"cta\": \"Comment with your experience\",\n",
    "          \"length_hint\": 900,\n",
    "          \"emoji\": True,\n",
    "      },\n",
    "      \"num_variants\": 2,\n",
    "      \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": 256},  # bump num_predict for longer drafts\n",
    "  }\n",
    "  \n",
    "r = httpx.post(f\"{BASE}/generate\", json=gen_payload, timeout=180.0)\n",
    "r.raise_for_status()\n",
    "g = r.json()\n",
    "  \n",
    "gen_id = g[\"generation_id\"]\n",
    "chosen_text = g[\"chosen\"][\"text\"].strip()\n",
    "preview = textwrap.shorten(chosen_text.replace(\"\\n\", \" / \"), width=160)\n",
    "  \n",
    "print(f\"✓ Generated {len(g['variants'])} variants (generation_id={gen_id})\")\n",
    "print(\"Chosen preview:\", preview)\n",
    "print(\"Scores:\")\n",
    "for i, v in enumerate(g[\"variants\"]):\n",
    "    s = v[\"score\"]\n",
    "    print(f\"  v{i}: style={s['style_similarity']:.3f} nov={s['novelty']:.3f} length_ok={s['length_ok']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b09af",
   "metadata": {},
   "source": [
    "# Step 5 — show full variant texts (chosen + all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2fb3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chosen Variant ===\n",
      "Here's a draft for a LinkedIn post:\n",
      "\n",
      "\"📊 As an engineering manager, I've seen my fair share of onboarding disasters. But after our recent revamp, I'm convinced: short, friendly checklists are the way to go! 🚀 Gone are the days of lengthy docs that leave new devs feeling lost.\n",
      "\n",
      "In our previous state, it took an average dev team member 3-4 weeks to get up and running. Now? We're talking a swift 1-2 weeks. The difference? Clear, actionable steps in a simple checklist. No more sifting through unnecessary details! 👀\n",
      "\n",
      "So, what's your experience with onboarding? Have you seen similar results or struggled with lengthy docs? Comment below with your story! 💬 #DevOps #EngineeringManagement #OnboardingBestPractices\"\n",
      "\n",
      "Let me know if this meets your requirements! 😊\n",
      "\n",
      "Chosen score: {'style_similarity': 0.33007124066352844, 'novelty': 0.7634750455617905, 'structure_ok': True, 'length_ok': True}\n",
      "\n",
      "=== All Variants ===\n",
      "\n",
      "================================================================================\n",
      "Variant 0  |  length=760 chars\n",
      "Here's a draft for a LinkedIn post:\n",
      "\n",
      "\"📊 As an engineering manager, I've seen my fair share of onboarding disasters. But after our recent revamp, I'm convinced: short, friendly checklists are the way to go! 🚀 Gone are the days of lengthy docs that leave new devs feeling lost.\n",
      "\n",
      "In our previous state, it took an average dev team member 3-4 weeks to get up and running. Now? We're talking a swift 1-2 weeks. The difference? Clear, actionable steps in a simple checklist. No more sifting through unnecessary details! 👀\n",
      "\n",
      "So, what's your experience with onboarding? Have you seen similar results or struggled with lengthy docs? Comment below with your story! 💬 #DevOps #EngineeringManagement #OnboardingBestPractices\"\n",
      "\n",
      "Let me know if this meets your requirements! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.33007124066352844, 'novelty': 0.7634750455617905, 'structure_ok': True, 'length_ok': True}\n",
      "\n",
      "================================================================================\n",
      "Variant 1  |  length=666 chars\n",
      "Here is a draft LinkedIn post:\n",
      "\n",
      "\"Onboarding devs? 🚀 Make it a smooth ride! I learned this the hard way when my team's onboarding process was like trying to drink from a firehose 😅. Long docs, endless questions... not exactly the best use of their (or your) time.\n",
      "\n",
      "So, what worked? Short, friendly checklists that got them up and running in no time! And here's the thing: it's not just about getting devs on board - it's about setting them up for success from day one. 📈\n",
      "\n",
      "Hashtags: #DevOnboarding #EngineeringManagement #TeamProductivity\n",
      "\n",
      "CTA: Comment with your experience - what worked (or didn't work) in your dev team's onboarding process? Let's learn together! 💬\"\n",
      "\n",
      "Score: {'style_similarity': 0.33483850955963135, 'novelty': 0.7073153257369995, 'structure_ok': True, 'length_ok': True}\n"
     ]
    }
   ],
   "source": [
    "# Step 5: display full texts and scores\n",
    "  # (If NameError: g is not defined, re-run Step 4)\n",
    "  \n",
    "print(\"=== Chosen Variant ===\")\n",
    "print(g[\"chosen\"][\"text\"])\n",
    "print(\"\\nChosen score:\", g[\"chosen\"][\"score\"])\n",
    "  \n",
    "print(\"\\n=== All Variants ===\")\n",
    "for i, v in enumerate(g[\"variants\"]):\n",
    "      print(\"\\n\" + \"=\"*80)\n",
    "      print(f\"Variant {i}  |  length={len(v['text'])} chars\")\n",
    "      print(v[\"text\"])\n",
    "      print(\"\\nScore:\", v[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99d11e",
   "metadata": {},
   "source": [
    "# Step 6 — stream a live draft (prints tokens as they arrive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e875f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:40850 - \"POST /generate/stream HTTP/1.1\" 200 OK\n",
      "SSE connected\n",
      "\n",
      "[META]\n",
      "{'exemplars': ['Shipped our onboarding revamp. Short, friendly checklists beat long docs.', \"If your standup drags, it's a smell. Keep it under 10 minutes, tops.\", \"Docs are a product. If you don't version them, they'll version you.\"], 'goal': 'educate', 'keywords': ['onboarding', 'dev teams']}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 16:42:05,647 INFO [stylgen] generate.stream.request user_id=u_nb1 goal=educate keywords=onboarding,dev teams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a draft:\"Onboarding dev teams is like building a LEGO castle - it takes precision, patience, and a solid foundation 🏰. I've seen too many new hires get lost in sea of documentation or left dangling with unclear expectations.So, what works? For me, it's about setting clear goals and priorities up front. A concise onboarding plan beats a lengthy manual any day! And don't even get me started on the power of regular check-ins - it's like having a trusty LEGO instruction book 📚#EngineeringManagement #DevTeamOnboarding #AgileLessons\"Feel free to comment with your experience!\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: streaming generation (SSE)\n",
    "import asyncio, httpx\n",
    "  \n",
    "async def stream_post():\n",
    "      req = {\n",
    "          \"user_id\": user_id,  # from Step 3\n",
    "          \"brief\": {\n",
    "              \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "              \"goal\": \"educate\",\n",
    "              \"audience\": \"engineering managers\",\n",
    "              \"cta\": \"Comment with your experience\",\n",
    "              \"length_hint\": 900,\n",
    "              \"emoji\": True,\n",
    "          },\n",
    "          \"num_variants\": 1,\n",
    "          \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": 256},\n",
    "      }\n",
    "      async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "          async with client.stream(\"POST\", f\"{BASE}/generate/stream\", json=req) as resp:\n",
    "              resp.raise_for_status()\n",
    "              print(\"SSE connected\")\n",
    "              printed = 0\n",
    "              async for line in resp.aiter_lines():\n",
    "                  if not line:\n",
    "                      continue\n",
    "                  if line.startswith(\"event: meta\"):\n",
    "                      print(\"\\n[META]\")\n",
    "                  elif line.startswith(\"event: done\"):\n",
    "                      print(\"\\n[done]\")\n",
    "                      break\n",
    "                  elif line.startswith(\"data: \"):\n",
    "                      chunk = line[len(\"data: \"):]\n",
    "                      print(chunk, end=\"\", flush=True)\n",
    "                      printed += len(chunk)\n",
    "                      if printed > 1200:\n",
    "                          print(\"\\n[truncated display after ~1200 chars]\")\n",
    "                          # keep draining until 'done' event\n",
    "              # end for\n",
    "  \n",
    "await stream_post()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0aa27",
   "metadata": {},
   "source": [
    "# Step 7 — submit feedback for the chosen generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa56823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:51604 - \"POST /feedback HTTP/1.1\" 200 OK\n",
      "Feedback response: {'status': 'received'}\n"
     ]
    }
   ],
   "source": [
    "# Step 7: submit feedback (positive case)\n",
    "import httpx, json\n",
    "  \n",
    "assert \"g\" in globals(), \"Please run Step 4 to create a generation first.\"\n",
    "fb_payload = {\n",
    "      \"user_id\": user_id,                 # same as used to generate\n",
    "      \"generation_id\": g[\"generation_id\"],# from Step 4\n",
    "      \"rating\": 4,                        # 1..5\n",
    "      \"tags\": [\"good tone\", \"useful CTA\"]\n",
    "  }\n",
    "r = httpx.post(f\"{BASE}/feedback\", json=fb_payload, timeout=10.0)\n",
    "r.raise_for_status()\n",
    "print(\"Feedback response:\", r.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00497cbe",
   "metadata": {},
   "source": [
    "# END of normal process with HashingEmbedder. next we try sentence-transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadb45f",
   "metadata": {},
   "source": [
    "# Step 8a — ensure sentence-transformers is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bdb8945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: uv sync --extra hf-embeddings (cwd: /home/gluttony47/WORK/personal/claude_pogo/stylgen_v0 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m75 packages\u001b[0m \u001b[2min 0.85ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m networkx \u001b[2m(1.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pillow \u001b[2m(6.3MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m setuptools \u001b[2m(1.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-nvjitlink-cu12 \u001b[2m(37.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m scipy \u001b[2m(33.5MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m scikit-learn \u001b[2m(9.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m hf-xet \u001b[2m(3.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cudnn-cu12 \u001b[2m(674.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m triton \u001b[2m(148.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torch \u001b[2m(846.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-curand-cu12 \u001b[2m(60.7MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cusolver-cu12 \u001b[2m(255.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cufft-cu12 \u001b[2m(184.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cusparselt-cu12 \u001b[2m(273.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-nccl-cu12 \u001b[2m(307.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cusparse-cu12 \u001b[2m(274.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cufile-cu12 \u001b[2m(1.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m transformers \u001b[2m(11.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cuda-cupti-cu12 \u001b[2m(9.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cublas-cu12 \u001b[2m(566.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers \u001b[2m(3.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cuda-nvrtc-cu12 \u001b[2m(84.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sympy \u001b[2m(6.0MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m setuptools\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cufile-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m networkx\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m hf-xet\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m sympy\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pillow\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m scikit-learn\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cuda-cupti-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m transformers\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m scipy\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-nvjitlink-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-curand-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cuda-nvrtc-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m triton\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cufft-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cusolver-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cusparselt-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cusparse-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-nccl-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cublas-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m nvidia-cudnn-cu12\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m torch\n",
      "\u001b[2mPrepared \u001b[1m40 packages\u001b[0m \u001b[2min 2m 28s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m27 packages\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uv sync complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mInstalled \u001b[1m40 packages\u001b[0m \u001b[2min 177ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.16\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.19.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.34.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.30.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.4.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.0.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==5.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 8a: install optional extra (hf-embeddings) with uv\n",
    "import os, shutil, subprocess\n",
    "  \n",
    "# Find repo root (where pyproject.toml lives)\n",
    "root = os.getcwd()\n",
    "if not os.path.exists(os.path.join(root, \"pyproject.toml\")):\n",
    "      root = os.path.abspath(os.path.join(root, \"..\"))\n",
    "  \n",
    "assert shutil.which(\"uv\"), \"uv CLI not found on PATH. Install uv, then retry.\"\n",
    "print(\"Running: uv sync --extra hf-embeddings (cwd:\", root, \")\")\n",
    "subprocess.check_call([\"uv\", \"sync\", \"--extra\", \"hf-embeddings\"], cwd=root)\n",
    "print(\"uv sync complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64cc42",
   "metadata": {},
   "source": [
    "# Step 8b — restart server with ST embedder (e5-large-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dad1616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [18355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server PID: 19129 BASE: http://127.0.0.1:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:00:54,317 INFO [stylgen] embedder.selected kind=st model=intfloat/e5-large-v2\n",
      "INFO:     Started server process [19129]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Step 8b: restart server with ST embedder (e5-large-v2)\n",
    "import os, sys, time, subprocess\n",
    "  \n",
    "# Stop previous server\n",
    "try:\n",
    "      SERVER_PROC\n",
    "except NameError:\n",
    "      SERVER_PROC = None\n",
    "  \n",
    "if SERVER_PROC and SERVER_PROC.poll() is None:\n",
    "      SERVER_PROC.terminate()\n",
    "      try:\n",
    "          SERVER_PROC.wait(timeout=5)\n",
    "      except Exception:\n",
    "          SERVER_PROC.kill()\n",
    "  \n",
    "HOST, PORT = \"127.0.0.1\", 8000\n",
    "BASE = f\"http://{HOST}:{PORT}\"\n",
    "  \n",
    "env = os.environ.copy()\n",
    "env[\"STYLGEN_LOG_LEVEL\"] = \"INFO\"\n",
    "env[\"STYLGEN_DEBUG\"] = \"0\"\n",
    "env[\"STYLGEN_EMBEDDER\"] = \"st\"\n",
    "env[\"STYLGEN_ST_MODEL\"] = \"intfloat/e5-large-v2\"  # bigger, higher quality\n",
    "  \n",
    "cmd = [sys.executable, \"-m\", \"uvicorn\", \"stylgen_v0.main:app\",\n",
    "         \"--host\", HOST, \"--port\", str(PORT), \"--log-level\", \"info\"]\n",
    "SERVER_PROC = subprocess.Popen(cmd, env=env)\n",
    "print(\"Server PID:\", SERVER_PROC.pid, \"BASE:\", BASE)\n",
    "time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a2b6f",
   "metadata": {},
   "source": [
    "# Step 8c — health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512e9908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52452 - \"GET /health HTTP/1.1\" 200 OK\n",
      "Health: 200 {'status': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "# Step 8c: health check\n",
    "import time, httpx\n",
    "for _ in range(40):\n",
    "      try:\n",
    "          r = httpx.get(f\"{BASE}/health\", timeout=2.0)\n",
    "          print(\"Health:\", r.status_code, r.json())\n",
    "          break\n",
    "      except Exception:\n",
    "          time.sleep(0.25)\n",
    "else:\n",
    "      raise RuntimeError(\"Server did not become healthy in time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5f618",
   "metadata": {},
   "source": [
    "# Step 9 — re-create persona (fresh in-memory store after restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f15092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:02:20,532 INFO [stylgen] persona.create user_id=u_nb1_st samples=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:56748 - \"POST /persona HTTP/1.1\" 200 OK\n",
      "✓ Persona created (ST)\n",
      "user_id: u_nb1_st num_samples: 3\n",
      "centroid_dim: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:02:20,795 INFO [stylgen] persona.created user_id=u_nb1_st exemplars=3 centroid=yes\n"
     ]
    }
   ],
   "source": [
    "# Step 9: create persona again (first ST call may take a minute to load model)\n",
    "import httpx\n",
    "  \n",
    "user_id = \"u_nb1_st\"  # new ID to keep runs separate\n",
    "persona_payload = {\n",
    "      \"user_id\": user_id,\n",
    "      \"samples\": [\n",
    "          \"Shipped our onboarding revamp. Short, friendly checklists beat long docs.\",\n",
    "          \"If your standup drags, it's a smell. Keep it under 10 minutes, tops.\",\n",
    "          \"Docs are a product. If you don't version them, they'll version you.\"\n",
    "      ],\n",
    "      \"preferences\": {\n",
    "          \"tone_descriptors\": [\"forthright\", \"practical\", \"lightly humorous\"],\n",
    "          \"taboo_phrases\": [\"In today's fast-paced world\"],\n",
    "          \"formality\": 2,\n",
    "          \"emoji_ok\": True,\n",
    "          \"hashtags_niche\": True,\n",
    "          \"structure_pref\": \"story-first\"\n",
    "      }\n",
    "  }\n",
    "r = httpx.post(f\"{BASE}/persona\", json=persona_payload, timeout=180.0)\n",
    "r.raise_for_status()\n",
    "resp = r.json()\n",
    "card = resp[\"persona\"]\n",
    "print(\"✓ Persona created (ST)\")\n",
    "print(\"user_id:\", resp[\"user_id\"], \"num_samples:\", resp[\"num_samples\"])\n",
    "print(\"centroid_dim:\", None if card[\"centroid\"] is None else len(card[\"centroid\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff760c6c",
   "metadata": {},
   "source": [
    "# Step 10 — generate 2 variants (with ST embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c3a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:04:30,067 INFO [stylgen] generate.request user_id=u_nb1_st goal=educate keywords=onboarding,dev teams variants=3 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:46678 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "✓ Generated 3 variants (generation_id=62c6a40f-1cf1-4d2b-bbb4-53016c3c5c31)\n",
      "  v0: style=0.886 nov=0.134 length_ok=False\n",
      "  v1: style=0.877 nov=0.124 length_ok=True\n",
      "  v2: style=0.865 nov=0.138 length_ok=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:04:36,464 INFO [stylgen.pipeline] variants.sorted count=3 top_sim=0.886 top_nov=0.134 length_ok=False\n",
      "2025-09-06 17:04:36,464 INFO [stylgen] generate.done user_id=u_nb1_st generation_id=62c6a40f-1cf1-4d2b-bbb4-53016c3c5c31 chosen_sim=0.886 chosen_novelty=0.134\n"
     ]
    }
   ],
   "source": [
    "# Step 10: generate (ST)\n",
    "import httpx, textwrap\n",
    "  \n",
    "gen_payload = {\n",
    "      \"user_id\": user_id,  # \"u_nb1_st\" from Step 9\n",
    "      \"brief\": {\n",
    "          \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "          \"goal\": \"educate\",\n",
    "          \"audience\": \"engineering managers\",\n",
    "          \"cta\": \"Comment with your experience\",\n",
    "          \"length_hint\": 900,\n",
    "          \"emoji\": True,\n",
    "      },\n",
    "      \"num_variants\": 3,\n",
    "      \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": 256},\n",
    "  }\n",
    "  \n",
    "r = httpx.post(f\"{BASE}/generate\", json=gen_payload, timeout=180.0)\n",
    "r.raise_for_status()\n",
    "g_st = r.json()\n",
    "  \n",
    "print(f\"✓ Generated {len(g_st['variants'])} variants (generation_id={g_st['generation_id']})\")\n",
    "for i, v in enumerate(g_st[\"variants\"]):\n",
    "      s = v[\"score\"]\n",
    "      print(f\"  v{i}: style={s['style_similarity']:.3f} nov={s['novelty']:.3f} length_ok={s['length_ok']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67109fe",
   "metadata": {},
   "source": [
    "# Step 11 — show full texts (chosen + all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94557e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chosen Variant (ST) ===\n",
      "Here's my attempt:\n",
      "\n",
      "\"Just finished onboarding our new dev team members 🤩! I've learned that short, snappy checklists > lengthy docs. Our new crew is up to speed in no time. Want to know my secret? 🤔 It starts with a clear plan, not a 50-page manual. Try it out and let me know your experience! #devops #onboardinghacks\"\n",
      "\n",
      "Let me know if this meets your expectations or if I need to make any changes! 😊\n",
      "\n",
      "Chosen score: {'style_similarity': 0.885976254940033, 'novelty': 0.1336246132850647, 'structure_ok': True, 'length_ok': False}\n",
      "\n",
      "=== All Variants (ST) ===\n",
      "\n",
      "================================================================================\n",
      "Variant 0  |  length=400 chars\n",
      "Here's my attempt:\n",
      "\n",
      "\"Just finished onboarding our new dev team members 🤩! I've learned that short, snappy checklists > lengthy docs. Our new crew is up to speed in no time. Want to know my secret? 🤔 It starts with a clear plan, not a 50-page manual. Try it out and let me know your experience! #devops #onboardinghacks\"\n",
      "\n",
      "Let me know if this meets your expectations or if I need to make any changes! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.885976254940033, 'novelty': 0.1336246132850647, 'structure_ok': True, 'length_ok': False}\n",
      "\n",
      "================================================================================\n",
      "Variant 1  |  length=581 chars\n",
      "Here's a draft:\n",
      "\n",
      "\"Just shipped our onboarding overhaul 🚀! Dev teams know the drill: new hires take forever to get up to speed because of lengthy docs and tedious setup processes. I'm guilty too - until I realized that short, friendly checklists can make all the difference. Our revamped onboarding now takes 1/3rd the time and leaves room for actual work 💪. So, what's your worst onboarding horror story? 🤔 Comment with your experience! #engineeringmanagement #devops #onboardingbestpractices\"\n",
      "\n",
      "Let me know if this meets your requirements or if you'd like me to make any changes! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.8774722814559937, 'novelty': 0.12359541654586792, 'structure_ok': True, 'length_ok': True}\n",
      "\n",
      "================================================================================\n",
      "Variant 2  |  length=916 chars\n",
      "Here's a draft:\n",
      "\n",
      "\"Confession time: our dev team used to spend hours onboarding new engineers... until we rewrote the rulebook 📝. Gone are the days of overwhelming documentation; hello, bite-sized checklists! 💪\n",
      "\n",
      "I recall one instance where a checklist saved the day (literally). A new engineer was struggling with setting up their IDE. After 5 minutes of searching, they stumbled upon our 'Quick Start' guide and were coding like pros in no time 🚀.\n",
      "\n",
      "So, what's the takeaway? Onboarding doesn't have to be an ordeal. By streamlining processes and focusing on actionable steps, you can empower your dev teams to hit the ground running. Try it out and see the magic happen! ✨\n",
      "\n",
      "Comment with your experience: What's the most effective way you've onboarded new engineers in your team? #EngineeringOnboarding #DevTeamEssentials #AgileLearning\"\n",
      "\n",
      "Let me know if this meets your requirements or if I need to make any changes! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.8651975393295288, 'novelty': 0.13831257820129395, 'structure_ok': True, 'length_ok': True}\n"
     ]
    }
   ],
   "source": [
    "# Step 11: display full texts (ST)\n",
    "print(\"=== Chosen Variant (ST) ===\")\n",
    "print(g_st[\"chosen\"][\"text\"])\n",
    "print(\"\\nChosen score:\", g_st[\"chosen\"][\"score\"])\n",
    "  \n",
    "print(\"\\n=== All Variants (ST) ===\")\n",
    "for i, v in enumerate(g_st[\"variants\"]):\n",
    "      print(\"\\n\" + \"=\"*80)\n",
    "      print(f\"Variant {i}  |  length={len(v['text'])} chars\")\n",
    "      print(v[\"text\"])\n",
    "      print(\"\\nScore:\", v[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929a7f1",
   "metadata": {},
   "source": [
    "# Step 12 — stream a live draft (with ST embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15947806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:56420 - \"POST /generate/stream HTTP/1.1\" 200 OK\n",
      "SSE connected (ST)\n",
      "\n",
      "[META] {'exemplars': ['Shipped our onboarding revamp. Short, friendly checklists beat long docs.', \"If your standup drags, it's a smell. Keep it under 10 minutes, tops.\", \"Docs are a product. If you don't version them, they'll version you.\"], 'goal': 'educate', 'keywords': ['onboarding', 'dev teams']}\n",
      "Here's a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:08:32,468 INFO [stylgen] generate.stream.request user_id=u_nb1_st goal=educate keywords=onboarding,dev teams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " draft:Shipped our onboarding revamp 🚀! I've learned that dev teams can't afford to waste time on unnecessary docs. Instead, I use short, friendly checklists that get new team members up and running in no time.I recall one engineer who took 3 weeks to onboard because of lengthy documentation. That's 15 working days of productivity lost 🕰️! Now, we've cut the onboarding process by 75% using our new approach.Want to learn how you can streamline your dev team onboarding? Comment with your experience and let's discuss best practices 👇 #EngineeringManagementTips #DevTeamOnboarding #SoftwareDevelopmentBestPractices\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "# Step 12: streaming generation (SSE) with sentence-transformers persona\n",
    "  # Note: streamed text is raw (no critique step applied while streaming).\n",
    "import asyncio, httpx\n",
    "  \n",
    "async def stream_post_st():\n",
    "      req = {\n",
    "          \"user_id\": user_id,  # \"u_nb1_st\" from Step 9\n",
    "          \"brief\": {\n",
    "              \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "              \"goal\": \"educate\",\n",
    "              \"audience\": \"engineering managers\",\n",
    "              \"cta\": \"Comment with your experience\",\n",
    "              \"length_hint\": 900,\n",
    "              \"emoji\": True,\n",
    "          },\n",
    "          \"num_variants\": 1,\n",
    "          \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": 256},\n",
    "      }\n",
    "      async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "          async with client.stream(\"POST\", f\"{BASE}/generate/stream\", json=req) as resp:\n",
    "              resp.raise_for_status()\n",
    "              print(\"SSE connected (ST)\")\n",
    "              last_event = None\n",
    "              printed = 0\n",
    "              async for line in resp.aiter_lines():\n",
    "                  if not line:\n",
    "                      continue\n",
    "                  if line.startswith(\"event: \"):\n",
    "                      last_event = line.split(\"event: \", 1)[1].strip()\n",
    "                      if last_event == \"meta\":\n",
    "                          print(\"\\n[META]\", end=\" \")\n",
    "                      elif last_event == \"done\":\n",
    "                          print(\"\\n[done]\")\n",
    "                          break\n",
    "                      continue\n",
    "                  if line.startswith(\"data: \"):\n",
    "                      data = line[len(\"data: \"):]\n",
    "                      if last_event == \"meta\":\n",
    "                          print(data)  # prints exemplar previews and brief meta\n",
    "                          last_event = None\n",
    "                      else:\n",
    "                          print(data, end=\"\", flush=True)\n",
    "                          printed += len(data)\n",
    "                          if printed > 1200:\n",
    "                              print(\"\\n[truncated display after ~1200 chars]\")\n",
    "                              # keep draining until 'done' event\n",
    "              # end for\n",
    "  \n",
    "await stream_post_st()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48805404",
   "metadata": {},
   "source": [
    "# Step 13 — create your own persona (edit samples, then run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b97ffa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55664 - \"POST /persona HTTP/1.1\" 200 OK\n",
      "✓ Persona created: u_custom1 samples: 3\n",
      "exemplar_ids: ['524b5bb3-6211-4fd4-bf54-f2d032485aca', 'ed5aa321-2ba9-4460-acae-ef6a9ffd7ca2', '7afe07d9-39b3-4111-a60f-f284861e3465']\n",
      "centroid_dim: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:18:11,540 INFO [stylgen] persona.create user_id=u_custom1 samples=3\n",
      "2025-09-06 17:18:11,632 INFO [stylgen] persona.created user_id=u_custom1 exemplars=3 centroid=yes\n"
     ]
    }
   ],
   "source": [
    "# Step 13: create your own persona (edit samples below)\n",
    "  \n",
    "import httpx\n",
    "  \n",
    "user_id_custom = \"u_custom1\"  # change if you like\n",
    "custom_samples = [\n",
    "      \"Your sample post 1 (paste real text here).\",\n",
    "      \"Your sample post 2 (paste real text here).\",\n",
    "      \"Your sample post 3 (paste real text here).\",\n",
    "  ]\n",
    "preferences_custom = {\n",
    "      \"tone_descriptors\": [\"direct\", \"witty\", \"contrarian\"],\n",
    "      \"taboo_phrases\": [\"As an AI\", \"In today's fast-paced world\"],\n",
    "      \"formality\": 2,\n",
    "      \"emoji_ok\": True,\n",
    "      \"hashtags_niche\": True,\n",
    "      \"structure_pref\": \"story-first\",\n",
    "  }\n",
    "  \n",
    "r = httpx.post(f\"{BASE}/persona\", json={\n",
    "      \"user_id\": user_id_custom,\n",
    "      \"samples\": custom_samples,\n",
    "      \"preferences\": preferences_custom\n",
    "  }, timeout=180.0)\n",
    "r.raise_for_status()\n",
    "resp_c = r.json()\n",
    "print(\"✓ Persona created:\", resp_c[\"user_id\"], \"samples:\", resp_c[\"num_samples\"])\n",
    "print(\"exemplar_ids:\", resp_c[\"persona\"][\"exemplar_ids\"])\n",
    "print(\"centroid_dim:\", None if resp_c[\"persona\"][\"centroid\"] is None else len(resp_c[\"persona\"]\n",
    "  [\"centroid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2214de",
   "metadata": {},
   "source": [
    "# Step 14 — generate for your custom persona (non-stream) and preview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f93526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:20:39,890 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=2 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57082 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "✓ Generated 2 variants (generation_id=b18c7669-a07d-4348-9187-7b5e8063b309)\n",
      "  v0: style=0.822 nov=0.181 length_ok=True\n",
      "  v1: style=0.786 nov=0.213 length_ok=True\n",
      "Chosen preview: Here's my attempt at crafting a LinkedIn post: / / \"Confession time, fellow engineering managers: I'm guilty of making dev teams feel like they're trying [...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:20:45,288 INFO [stylgen.pipeline] variants.sorted count=2 top_sim=0.822 top_nov=0.181 length_ok=True\n",
      "2025-09-06 17:20:45,289 INFO [stylgen] generate.done user_id=u_custom1 generation_id=b18c7669-a07d-4348-9187-7b5e8063b309 chosen_sim=0.822 chosen_novelty=0.181\n"
     ]
    }
   ],
   "source": [
    "# Step 14: generate for your custom persona\n",
    "import httpx, textwrap\n",
    "  \n",
    "assert \"user_id_custom\" in globals(), \"Please run Step 13 first.\"\n",
    "  \n",
    "gen_payload = {\n",
    "      \"user_id\": user_id_custom,\n",
    "      \"brief\": {\n",
    "          \"keywords\": [\"onboarding\", \"dev teams\"],  # edit if you like\n",
    "          \"goal\": \"educate\",\n",
    "          \"audience\": \"engineering managers\",\n",
    "          \"cta\": \"Comment with your experience\",\n",
    "          \"length_hint\": 900,\n",
    "          \"emoji\": True,\n",
    "      },\n",
    "      \"num_variants\": 2,\n",
    "      \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": 256},\n",
    "  }\n",
    "  \n",
    "r = httpx.post(f\"{BASE}/generate\", json=gen_payload, timeout=180.0)\n",
    "r.raise_for_status()\n",
    "g_custom = r.json()\n",
    "  \n",
    "print(f\"✓ Generated {len(g_custom['variants'])} variants (generation_id={g_custom['generation_id']})\")\n",
    "for i, v in enumerate(g_custom[\"variants\"]):\n",
    "      s = v[\"score\"]\n",
    "      print(f\"  v{i}: style={s['style_similarity']:.3f} nov={s['novelty']:.3f} length_ok={s['length_ok']}\")\n",
    "print(\"Chosen preview:\", textwrap.shorten(g_custom[\"chosen\"][\"text\"].replace(\"\\n\", \" / \"),\n",
    "  width=160))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbde76",
   "metadata": {},
   "source": [
    "# Step 15 — display full texts (custom persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7205aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chosen Variant (custom) ===\n",
      "Here's my attempt at crafting a LinkedIn post:\n",
      "\n",
      "\"Confession time, fellow engineering managers: I'm guilty of making dev teams feel like they're trying to solve a puzzle blindfolded during onboarding 🤯. How many times have we seen talented engineers struggle to find their footing, only to realize that the issue wasn't the tech, but the lack of context and clear expectations? 💡\n",
      "\n",
      "As someone who's been there, done that, and got the t-shirt (literally), I've learned a thing or two about streamlining onboarding. Want to learn more? 🤔 Comment with your experience below! #DevOps #OnboardingMistakes\"\n",
      "\n",
      "Let me know if this meets your expectations! 😊\n",
      "\n",
      "Chosen score: {'style_similarity': 0.821702241897583, 'novelty': 0.1814666986465454, 'structure_ok': True, 'length_ok': True}\n",
      "\n",
      "=== All Variants (custom) ===\n",
      "\n",
      "================================================================================\n",
      "Variant 0  |  length=646 chars\n",
      "Here's my attempt at crafting a LinkedIn post:\n",
      "\n",
      "\"Confession time, fellow engineering managers: I'm guilty of making dev teams feel like they're trying to solve a puzzle blindfolded during onboarding 🤯. How many times have we seen talented engineers struggle to find their footing, only to realize that the issue wasn't the tech, but the lack of context and clear expectations? 💡\n",
      "\n",
      "As someone who's been there, done that, and got the t-shirt (literally), I've learned a thing or two about streamlining onboarding. Want to learn more? 🤔 Comment with your experience below! #DevOps #OnboardingMistakes\"\n",
      "\n",
      "Let me know if this meets your expectations! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.821702241897583, 'novelty': 0.1814666986465454, 'structure_ok': True, 'length_ok': True}\n",
      "\n",
      "================================================================================\n",
      "Variant 1  |  length=943 chars\n",
      "Here's a draft post:\n",
      "\n",
      "\"As an engineering manager, I've seen my fair share of onboarding disasters 🚨. But let me tell you, it doesn't have to be a nightmare. In fact, when done right, onboarding can set your dev teams up for success from day one.\n",
      "\n",
      "I recall one instance where we nailed it: we sent our new team member a personalized welcome package with a simple task list and a clear roadmap. Voilà! They were contributing within weeks 🕒.\n",
      "\n",
      "So, what's the secret sauce? It's not about creating a fancy onboarding program (although that helps). It's about setting realistic expectations, establishing open communication channels, and making sure everyone knows their role in the grand scheme of things.\n",
      "\n",
      "Now it's your turn! What's your most memorable onboarding experience as an engineering manager? Comment below with your story 📝. #EngineeringManagementTips #DevTeamOnboarding #ManagerLife\"\n",
      "\n",
      "Let me know if you'd like me to make any changes! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.7864453792572021, 'novelty': 0.2133558988571167, 'structure_ok': True, 'length_ok': True}\n"
     ]
    }
   ],
   "source": [
    "# Step 15: display full texts (custom persona)\n",
    "assert \"g_custom\" in globals(), \"Please run Step 14 first.\"\n",
    "  \n",
    "print(\"=== Chosen Variant (custom) ===\")\n",
    "print(g_custom[\"chosen\"][\"text\"])\n",
    "print(\"\\nChosen score:\", g_custom[\"chosen\"][\"score\"])\n",
    "  \n",
    "print(\"\\n=== All Variants (custom) ===\")\n",
    "for i, v in enumerate(g_custom[\"variants\"]):\n",
    "      print(\"\\n\" + \"=\"*80)\n",
    "      print(f\"Variant {i}  |  length={len(v['text'])} chars\")\n",
    "      print(v[\"text\"])\n",
    "      print(\"\\nScore:\", v[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8c6b5",
   "metadata": {},
   "source": [
    "# Step 16 — compare temperature (0.2 vs 1.0) for your custom persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcae5169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:26:08,681 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n",
      "2025-09-06 17:26:12,612 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.791 top_nov=0.210 length_ok=True\n",
      "2025-09-06 17:26:12,612 INFO [stylgen] generate.done user_id=u_custom1 generation_id=599e31f9-fe89-4c46-a365-20c1cbdf14e3 chosen_sim=0.791 chosen_novelty=0.210\n",
      "2025-09-06 17:26:12,640 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:38544 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:38550 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "✓ Generated with different temperatures (seed=123)\n",
      "T=0.2 | len=912 | style=0.791 nov=0.210 length_ok=True\n",
      "   Here's a draft post: / / \"I've seen it time and again: dev teams stuck in onboarding limbo, with new hires struggling to get up to speed. 🚀 As an [...]\n",
      "T=1.0 | len=961 | style=0.805 nov=0.196 length_ok=True\n",
      "   Here's my attempt at crafting a LinkedIn post: / / \"I've seen it time and again: onboarding new devs to an existing team is like trying to cram a square [...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:26:14,557 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.805 top_nov=0.196 length_ok=True\n",
      "2025-09-06 17:26:14,557 INFO [stylgen] generate.done user_id=u_custom1 generation_id=a4cb33e4-7646-4549-9552-231b647f7ec6 chosen_sim=0.805 chosen_novelty=0.196\n"
     ]
    }
   ],
   "source": [
    "# Step 16: temperature comparison (same seed, different temperature)\n",
    "import httpx, textwrap\n",
    "  \n",
    "assert \"user_id_custom\" in globals(), \"Please run Step 13 first.\"\n",
    "  \n",
    "def gen_with_temp(temp: float):\n",
    "      r = httpx.post(\n",
    "          f\"{BASE}/generate\",\n",
    "          json={\n",
    "              \"user_id\": user_id_custom,\n",
    "              \"brief\": {\n",
    "                  \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "                  \"goal\": \"educate\",\n",
    "                  \"audience\": \"engineering managers\",\n",
    "                  \"cta\": \"Comment with your experience\",\n",
    "                  \"length_hint\": 900,\n",
    "                  \"emoji\": True,\n",
    "              },\n",
    "              \"num_variants\": 1,\n",
    "              \"llm_options\": {\"temperature\": temp, \"top_p\": 0.9, \"num_predict\": 256, \"seed\": 123},\n",
    "          },\n",
    "          timeout=180.0,\n",
    "      )\n",
    "      r.raise_for_status()\n",
    "      return r.json()\n",
    "  \n",
    "g_t02 = gen_with_temp(0.2)\n",
    "g_t10 = gen_with_temp(1.0)\n",
    "  \n",
    "def summarize(tag, g):\n",
    "      v = g[\"chosen\"]\n",
    "      s = v[\"score\"]\n",
    "      preview = textwrap.shorten(v[\"text\"].replace(\"\\n\", \" / \"), width=160)\n",
    "      print(f\"{tag} | len={len(v['text'])} | style={s['style_similarity']:.3f} nov={s['novelty']:.3f} length_ok={s['length_ok']}\")\n",
    "      print(\"  \", preview)\n",
    "  \n",
    "print(\"✓ Generated with different temperatures (seed=123)\")\n",
    "summarize(\"T=0.2\", g_t02)\n",
    "summarize(\"T=1.0\", g_t10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bf52d",
   "metadata": {},
   "source": [
    "# Step 17 — compare num_predict (output length and latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "818e81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:28:47,545 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n",
      "2025-09-06 17:28:48,859 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.814 top_nov=0.187 length_ok=True\n",
      "2025-09-06 17:28:48,859 INFO [stylgen] generate.done user_id=u_custom1 generation_id=9163d023-ad59-46f6-a838-e6e3d87bc0a3 chosen_sim=0.814 chosen_novelty=0.187\n",
      "2025-09-06 17:28:48,891 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50078 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:28:51,067 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.814 top_nov=0.186 length_ok=True\n",
      "2025-09-06 17:28:51,067 INFO [stylgen] generate.done user_id=u_custom1 generation_id=e418e5d5-7166-4530-ad6a-1af075ca17a8 chosen_sim=0.814 chosen_novelty=0.186\n",
      "2025-09-06 17:28:51,083 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50092 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:50098 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "num_predict=128 | len= 593 | style=0.814 | nov=0.187 | length_ok=True | 1352 ms\n",
      "num_predict=256 | len=1067 | style=0.814 | nov=0.186 | length_ok=True | 2207 ms\n",
      "num_predict=512 | len=1067 | style=0.814 | nov=0.186 | length_ok=True | 2249 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:28:53,316 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.814 top_nov=0.186 length_ok=True\n",
      "2025-09-06 17:28:53,317 INFO [stylgen] generate.done user_id=u_custom1 generation_id=552651d6-693a-4070-b773-0742c9a660c0 chosen_sim=0.814 chosen_novelty=0.186\n"
     ]
    }
   ],
   "source": [
    "# Step 17: num_predict comparison (128 vs 256 vs 512)\n",
    "import httpx, time\n",
    "  \n",
    "assert \"user_id_custom\" in globals(), \"Please run Step 13 first.\"\n",
    "  \n",
    "def gen_with_np(npred: int):\n",
    "      t0 = time.time()\n",
    "      r = httpx.post(\n",
    "          f\"{BASE}/generate\",\n",
    "          json={\n",
    "              \"user_id\": user_id_custom,\n",
    "              \"brief\": {\n",
    "                  \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "                  \"goal\": \"educate\",\n",
    "                  \"audience\": \"engineering managers\",\n",
    "                  \"cta\": \"Comment with your experience\",\n",
    "                  \"length_hint\": 900,\n",
    "                  \"emoji\": True,\n",
    "              },\n",
    "              \"num_variants\": 1,\n",
    "              \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": npred, \"seed\": 123},\n",
    "          },\n",
    "          timeout=180.0,\n",
    "      )\n",
    "      r.raise_for_status()\n",
    "      g = r.json()\n",
    "      dt_ms = int((time.time() - t0) * 1000)\n",
    "      v, s = g[\"chosen\"], g[\"chosen\"][\"score\"]\n",
    "      return {\"np\": npred, \"chars\": len(v[\"text\"]), \"style\": s[\"style_similarity\"], \"nov\":\n",
    "  s[\"novelty\"], \"length_ok\": s[\"length_ok\"], \"lat_ms\": dt_ms}\n",
    "  \n",
    "results = [gen_with_np(n) for n in (128, 256, 512)]\n",
    "for r in results:\n",
    "      print(f\"num_predict={r['np']:>3} | len={r['chars']:>4} | style={r['style']:.3f} | nov={r['nov']:.3f} | length_ok={r['length_ok']} | {r['lat_ms']} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499623ad",
   "metadata": {},
   "source": [
    "# Step 18 — length_hint=1500 vs num_predict (256 vs 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eb6221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:31:48,081 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=1500\n",
      "2025-09-06 17:31:49,988 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.801 top_nov=0.200 length_ok=False\n",
      "2025-09-06 17:31:49,988 INFO [stylgen] generate.done user_id=u_custom1 generation_id=db8fe59e-ad74-4fb6-99b3-3261ec154f93 chosen_sim=0.801 chosen_novelty=0.200\n",
      "2025-09-06 17:31:50,010 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:43194 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:31:52,362 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.794 top_nov=0.208 length_ok=True\n",
      "2025-09-06 17:31:52,362 INFO [stylgen] generate.done user_id=u_custom1 generation_id=fc060a00-03a1-4556-b7a6-5986140eb9aa chosen_sim=0.794 chosen_novelty=0.208\n",
      "2025-09-06 17:31:52,384 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:43198 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "len_hint=1500 | num_predict=256 | len= 737 | style=0.801 | nov=0.200 | length_ok=False | 1928 ms\n",
      "len_hint=1500 | num_predict=512 | len=1192 | style=0.794 | nov=0.208 | length_ok=True | 2373 ms\n",
      "INFO:     127.0.0.1:43212 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "{'np': 1024, 'chars': 1192, 'style': 0.7936976552009583, 'nov': 0.20806312561035156, 'length_ok': True, 'lat_ms': 2374}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:31:54,737 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.794 top_nov=0.208 length_ok=True\n",
      "2025-09-06 17:31:54,737 INFO [stylgen] generate.done user_id=u_custom1 generation_id=3838e1a2-08ad-4f85-b9de-26044ce344a9 chosen_sim=0.794 chosen_novelty=0.208\n"
     ]
    }
   ],
   "source": [
    "# Step 18: length_hint=1500 interacting with num_predict\n",
    "import httpx, time\n",
    "  \n",
    "assert \"user_id_custom\" in globals(), \"Please run Step 13 first.\"\n",
    "  \n",
    "def gen_len_hint(npred: int, hint: int = 1500):\n",
    "      t0 = time.time()\n",
    "      r = httpx.post(\n",
    "          f\"{BASE}/generate\",\n",
    "          json={\n",
    "              \"user_id\": user_id_custom,\n",
    "              \"brief\": {\n",
    "                  \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "                  \"goal\": \"educate\",\n",
    "                  \"audience\": \"engineering managers\",\n",
    "                  \"cta\": \"Comment with your experience\",\n",
    "                  \"length_hint\": hint,\n",
    "                  \"emoji\": True,\n",
    "              },\n",
    "              \"num_variants\": 1,\n",
    "              \"llm_options\": {\"temperature\": 0.7, \"top_p\": 0.9, \"num_predict\": npred, \"seed\": 123},\n",
    "          },\n",
    "          timeout=240.0,\n",
    "      )\n",
    "      r.raise_for_status()\n",
    "      g = r.json()\n",
    "      dt_ms = int((time.time() - t0) * 1000)\n",
    "      v, s = g[\"chosen\"], g[\"chosen\"][\"score\"]\n",
    "      return {\"np\": npred, \"chars\": len(v[\"text\"]), \"style\": s[\"style_similarity\"], \"nov\":\n",
    "  s[\"novelty\"], \"length_ok\": s[\"length_ok\"], \"lat_ms\": dt_ms}\n",
    "  \n",
    "results = [gen_len_hint(n) for n in (256, 512)]\n",
    "for r in results:\n",
    "      print(f\"len_hint=1500 | num_predict={r['np']:>3} | len={r['chars']:>4} | style={r['style']:.3f} | nov={r['nov']:.3f} | length_ok={r['length_ok']} | {r['lat_ms']} ms\")\n",
    "  \n",
    "# Optional heavier run:\n",
    "print(gen_len_hint(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3f698",
   "metadata": {},
   "source": [
    "# Step 19 — compare top_p (0.80 vs 0.95) at fixed temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df400ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:33:56,974 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n",
      "2025-09-06 17:33:59,412 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.813 top_nov=0.187 length_ok=True\n",
      "2025-09-06 17:33:59,413 INFO [stylgen] generate.done user_id=u_custom1 generation_id=4bf02041-c22b-47d2-8e4b-cc8a8feef8f2 chosen_sim=0.813 chosen_novelty=0.187\n",
      "2025-09-06 17:33:59,449 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42982 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42990 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "✓ Generated with top_p variants (temp=0.7, seed=123, len_hint=900, num_predict=256)\n",
      "top_p=0.80 | len=1140 | style=0.813 | nov=0.187 | length_ok=True | 2470 ms\n",
      "   Here's my attempt at crafting a LinkedIn post that fits your requirements: / / \"Confession time: I've seen more onboarding failures than successful ones [...]\n",
      "top_p=0.95 | len=712 | style=0.800 | nov=0.200 | length_ok=True | 1699 ms\n",
      "   Here's a draft LinkedIn post: / / \"As an engineering manager, I've seen my fair share of onboarding disasters 🚨. Remember that one dev team that took 6 [...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:34:01,124 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.800 top_nov=0.200 length_ok=True\n",
      "2025-09-06 17:34:01,124 INFO [stylgen] generate.done user_id=u_custom1 generation_id=b45dbb78-70c0-47fc-a795-72ba5027a942 chosen_sim=0.800 chosen_novelty=0.200\n"
     ]
    }
   ],
   "source": [
    "# Step 19: top_p comparison (hold temp/seed constant)\n",
    "import httpx, textwrap, time\n",
    "  \n",
    "assert \"user_id_custom\" in globals(), \"Please run Step 13 first.\"\n",
    "  \n",
    "def gen_with_topp(tp: float, hint: int = 900, temp: float = 0.7, npred: int = 256):\n",
    "      t0 = time.time()\n",
    "      r = httpx.post(\n",
    "          f\"{BASE}/generate\",\n",
    "          json={\n",
    "              \"user_id\": user_id_custom,\n",
    "              \"brief\": {\n",
    "                  \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "                  \"goal\": \"educate\",\n",
    "                  \"audience\": \"engineering managers\",\n",
    "                  \"cta\": \"Comment with your experience\",\n",
    "                  \"length_hint\": hint,\n",
    "                  \"emoji\": True,\n",
    "              },\n",
    "              \"num_variants\": 1,\n",
    "              \"llm_options\": {\"temperature\": temp, \"top_p\": tp, \"num_predict\": npred, \"seed\": 123},\n",
    "          },\n",
    "          timeout=180.0,\n",
    "      )\n",
    "      r.raise_for_status()\n",
    "      out = r.json()\n",
    "      dt_ms = int((time.time() - t0) * 1000)\n",
    "      v = out[\"chosen\"]\n",
    "      s = v[\"score\"]\n",
    "      preview = textwrap.shorten(v[\"text\"].replace(\"\\n\", \" / \"), width=160)\n",
    "      return {\n",
    "          \"tp\": tp, \"chars\": len(v[\"text\"]), \"style\": s[\"style_similarity\"], \"nov\": s[\"novelty\"],\n",
    "          \"length_ok\": s[\"length_ok\"], \"lat_ms\": dt_ms, \"preview\": preview\n",
    "      }\n",
    "  \n",
    "res_080 = gen_with_topp(0.80)\n",
    "res_095 = gen_with_topp(0.95)\n",
    "  \n",
    "print(\"✓ Generated with top_p variants (temp=0.7, seed=123, len_hint=900, num_predict=256)\")\n",
    "for r in (res_080, res_095):\n",
    "      print(f\"top_p={r['tp']:.2f} | len={r['chars']} | style={r['style']:.3f} | nov={r['nov']:.3f} | length_ok={r['length_ok']} | {r['lat_ms']} ms\")\n",
    "      print(\"  \", r[\"preview\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875cfe2",
   "metadata": {},
   "source": [
    "# Step 20 — show full texts for top_p 0.80 vs 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af08fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:36:56,671 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n",
      "2025-09-06 17:36:58,895 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.813 top_nov=0.192 length_ok=True\n",
      "2025-09-06 17:36:58,895 INFO [stylgen] generate.done user_id=u_custom1 generation_id=1fa567af-8373-4068-9767-de0cf270cc61 chosen_sim=0.813 chosen_novelty=0.192\n",
      "2025-09-06 17:36:58,924 INFO [stylgen] generate.request user_id=u_custom1 goal=educate keywords=onboarding,dev teams variants=1 len_hint=900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55422 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55428 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "=== top_p = 0.80 ===\n",
      "Here's my attempt at crafting a LinkedIn post that fits your requirements:\n",
      "\n",
      "\"Confession time: I've seen more onboarding disasters than I can count 🚨. As an engineering manager, you know how crucial it is to get dev teams up and running quickly. But let's face it, most onboarding processes are a mess of unnecessary meetings, unclear expectations, and too much repetition.\n",
      "\n",
      "My team and I have developed a 3-step approach that's been a for us: prioritize clear communication, automate repetitive tasks, and schedule regular check-ins. For example, we've reduced our average onboarding time by 30% simply by implementing a standardized template for new hires to follow 📈.\n",
      "\n",
      "So, what's your experience with onboarding? Have you found a process that works for you? Comment below and let's swap stories! #EngineeringManagement #DevTeamOnboarding #ManagerLife\"\n",
      "\n",
      "This post aims to educate engineering managers about the importance of effective onboarding, while also sharing a personal anecdote and encouraging others to share their own experiences. Let me know if this meets your requirements or if I need to make any adjustments!\n",
      "\n",
      "Score: {'style_similarity': 0.8132151365280151, 'novelty': 0.19222712516784668, 'structure_ok': True, 'length_ok': True}\n",
      "\n",
      "================================================================================\n",
      "=== top_p = 0.95 ===\n",
      "Here's a draft LinkedIn post:\n",
      "\n",
      "\"As an engineering manager, I've seen my fair share of onboarding disasters 🚨. Remember that one dev team that took 6 months to get a new hire up-to-speed? Yeah, let's not repeat those mistakes.\n",
      "\n",
      "My secret sauce? A well-crafted onboarding plan that prioritizes clarity and efficiency. Here's the key: treat each new team member like they're trying out for the same role (even if they're not). Give 'em clear goals, a solid understanding of the tech stack, and a friendly face to show 'em the ropes.\n",
      "\n",
      "So, what's your most epic onboarding story? Share it with me in the comments below! 👇 #EngineeringManagement #DevTeamOnboarding #TechTales\"\n",
      "\n",
      "Let me know if you'd like any changes! 😊\n",
      "\n",
      "Score: {'style_similarity': 0.8004229664802551, 'novelty': 0.19993466138839722, 'structure_ok': True, 'length_ok': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 17:37:00,573 INFO [stylgen.pipeline] variants.sorted count=1 top_sim=0.800 top_nov=0.200 length_ok=True\n",
      "2025-09-06 17:37:00,573 INFO [stylgen] generate.done user_id=u_custom1 generation_id=b59308fe-20b6-47f6-878e-4eb5a6d5093f chosen_sim=0.800 chosen_novelty=0.200\n"
     ]
    }
   ],
   "source": [
    "# Step 20: full chosen texts for top_p=0.80 and 0.95\n",
    "import httpx\n",
    "  \n",
    "assert \"user_id_custom\" in globals(), \"Please run Step 13 first.\"\n",
    "  \n",
    "def gen_topp_full(tp: float):\n",
    "      r = httpx.post(\n",
    "          f\"{BASE}/generate\",\n",
    "          json={\n",
    "              \"user_id\": user_id_custom,\n",
    "              \"brief\": {\n",
    "                  \"keywords\": [\"onboarding\", \"dev teams\"],\n",
    "                  \"goal\": \"educate\",\n",
    "                  \"audience\": \"engineering managers\",\n",
    "                  \"cta\": \"Comment with your experience\",\n",
    "                  \"length_hint\": 900,\n",
    "                  \"emoji\": True,\n",
    "              },\n",
    "              \"num_variants\": 1,\n",
    "              \"llm_options\": {\"temperature\": 0.7, \"top_p\": tp, \"num_predict\": 256, \"seed\": 123},\n",
    "          },\n",
    "          timeout=180.0,\n",
    "      )\n",
    "      r.raise_for_status()\n",
    "      return r.json()\n",
    "  \n",
    "g_tp080 = gen_topp_full(0.80)\n",
    "g_tp095 = gen_topp_full(0.95)\n",
    "  \n",
    "print(\"=== top_p = 0.80 ===\")\n",
    "print(g_tp080[\"chosen\"][\"text\"])\n",
    "print(\"\\nScore:\", g_tp080[\"chosen\"][\"score\"])\n",
    "  \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== top_p = 0.95 ===\")\n",
    "print(g_tp095[\"chosen\"][\"text\"])\n",
    "print(\"\\nScore:\", g_tp095[\"chosen\"][\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae949e",
   "metadata": {},
   "source": [
    "# other parameters to tweak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83c4f05",
   "metadata": {},
   "source": [
    "- LLM options: temperature, top_p, top_k, num_predict, repeat_penalty, seed, stop, num_ctx,\n",
    "  mirostat, mirostat_tau, mirostat_eta.\n",
    "- Brief: keywords, goal, audience, cta, length_hint, emoji, link.\n",
    "- Variants: num_variants (returns multiple and reranks).\n",
    "- Persona prefs: tone_descriptors, taboo_phrases, formality, emoji_ok, hashtags_niche,\n",
    "  structure_pref.\n",
    "- Embedder/model: STYLGEN_EMBEDDER, STYLGEN_ST_MODEL, OLLAMA_MODEL; logging via STYLGEN_DEBUG,\n",
    "  STYLGEN_LOG_LEVEL.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3b8bc",
   "metadata": {},
   "source": [
    "Notebook Walkthrough\n",
    "  \n",
    "  - Step 1: Start Server\n",
    "      - Purpose: Launches uvicorn stylgen_v0.main:app in the background with HashingEmbedder.\n",
    "      - Vars: HOST, PORT, BASE build the API base URL. SERVER_PROC tracks the spawned server so you\n",
    "  can terminate/restart it. env sets STYLGEN_LOG_LEVEL, STYLGEN_DEBUG, STYLGEN_EMBEDDER=hash.\n",
    "      - Behavior: If a previous SERVER_PROC exists, it’s terminated to avoid port conflicts. A new\n",
    "  process is started and PID printed.\n",
    "      - Behavior: If a previous SERVER_PROC exists, it’s terminated to avoid port conflicts. A new\n",
    "  process is started and PID printed.\n",
    "  - Step 1 (env tweak cell)\n",
    "      - Purpose: Sets env[\"OLLAMA_BASE\"] and env[\"OLLAMA_MODEL\"] in the notebook process.\n",
    "      - Note: This does not affect the already running server (env is read at process start). Use\n",
    "  these only before launching/restarting the server.\n",
    "  - Step 2: Health Check\n",
    "      - Purpose: Polls GET {BASE}/health up to ~10s until it returns {\"status\": \"ok\"}.\n",
    "      - Vars: BASE reused; httpx used synchronously for a simple check.\n",
    "      - Outcome: Confirms server is ready.\n",
    "  - Step 3: Create Persona (example)\n",
    "      - Purpose: Sends POST /persona with a sample user_id and three writing samples + preferences.\n",
    "      - Vars: user_id, persona_payload. Response saved as resp; card = resp[\"persona\"].\n",
    "      - Behavior: Server embeds samples, stores vectors, and builds a PersonaCard (centroid +\n",
    "  exemplar_ids). Prints exemplar_ids, centroid_dim (384 with hashing), and tone.\n",
    "  - Step 4: Generate (non‑stream)\n",
    "      - Purpose: Sends POST /generate with a brief, asks for 2 variants, forwards llm_options.\n",
    "      - Vars: gen_payload, response as g; extracts generation_id, g[\"variants\"], and g[\"chosen\"].\n",
    "      - Output: Prints chosen preview and per‑variant scores (style similarity, novelty, length_ok).\n",
    "  - Step 5: Show Full Variant Texts\n",
    "      - Purpose: Displays the entire text for the chosen variant and all variants with their scores.\n",
    "      - Vars: Uses g from Step 4.\n",
    "  - Step 6: Stream a Live Draft (SSE)\n",
    "      - Purpose: Streams raw tokens from POST /generate/stream and prints as they arrive.\n",
    "      - Vars: Async function stream_post(); builds req (same brief as Step 4). Tracks printed to\n",
    "  truncate display after ~1200 chars.\n",
    "      - Behavior: Prints an initial [META] event (exemplar previews and brief meta), then token\n",
    "  chunks (data: lines) until [done]. Streaming output is un‑critiqued (raw LLM text).\n",
    "  - Step 7: Submit Feedback\n",
    "      - Purpose: POST /feedback for the last generation.\n",
    "      - Vars: fb_payload uses user_id and g[\"generation_id\"]. Requires g to exist.\n",
    "      - Behavior: Server verifies generation exists and ownership matches. Returns\n",
    "  {\"status\":\"received\"}.\n",
    "  - Step 8a: Install Sentence‑Transformers (optional extra)\n",
    "      - Purpose: Runs uv sync --extra hf-embeddings from repo root to install the optional embeddings\n",
    "  stack.\n",
    "      - Behavior: Downloads heavy packages (Torch, Transformers, etc.) into your uv environment,\n",
    "  enabling ST embeddings.\n",
    "  - Step 8b: Restart with ST Embedder\n",
    "      - Purpose: Terminates the previous server and relaunches with STYLGEN_EMBEDDER=st and\n",
    "  STYLGEN_ST_MODEL=intfloat/e5-large-v2.\n",
    "      - Vars: Reuses SERVER_PROC, HOST/PORT/BASE, sets new env.\n",
    "      - Note: All in‑memory state is reset on restart; personas and generations must be recreated.\n",
    "  - Step 8c: Health Check (ST)\n",
    "      - Purpose: Same as Step 2, confirms the ST‑backed server is up.\n",
    "  - Step 9: Create Persona (ST)\n",
    "      - Purpose: Creates a new persona under user_id = \"u_nb1_st\".\n",
    "      - Output: centroid_dim is 1024 (from the ST model’s embedding size). Exemplars and centroid\n",
    "  computed with ST.\n",
    "  - Step 10: Generate (ST)\n",
    "      - Purpose: Generates with the ST persona; here num_variants=3 (note the increase vs earlier).\n",
    "      - Vars: Response saved to g_st.\n",
    "      - Output: Per‑variant scores; top may have length_ok=False depending on draft length. Sorting\n",
    "  favors style similarity + novelty, then length bonus.\n",
    "  - Step 11: Show Full Variant Texts (ST)\n",
    "      - Purpose: Displays g_st[\"chosen\"] and all g_st[\"variants\"] with scores.\n",
    "      - Observation: Higher style similarity values vs hashing are normal with a higher‑quality\n",
    "  embedder.\n",
    "  - Step 12: Stream (ST)\n",
    "      - Purpose: Same as Step 6, but with the ST persona (user_id = \"u_nb1_st\").\n",
    "      - Behavior: Prints [META] then streamed tokens until [done]. Uses last_event to distinguish\n",
    "  meta vs token chunks.\n",
    "  - Step 13: Create Your Own Persona\n",
    "      - Purpose: Lets you define user_id_custom, your custom_samples, and preferences_custom.\n",
    "      - Vars: Sends POST /persona with your data; prints exemplar_ids and centroid_dim (1024 under\n",
    "  ST).\n",
    "  - Step 14: Generate (Custom Persona)\n",
    "      - Purpose: Generates 2 variants for user_id_custom with the same brief defaults.\n",
    "      - Vars: Saves to g_custom. Prints per‑variant scores and a chosen preview.\n",
    "  - Step 15: Show Full Variant Texts (Custom Persona)\n",
    "      - Purpose: Displays chosen text and all variants for your persona, with scores.\n",
    "  - Step 16: Temperature Comparison (0.2 vs 1.0)\n",
    "      - Purpose: Compares outputs and scores at different temperature values with a fixed seed=123.\n",
    "      - Vars: gen_with_temp(temp) helper. Prints length, style/novelty, and a preview for each run.\n",
    "      - Note: seed helps reproducibility; Ollama forwards it at the provider layer.\n",
    "  - Step 17: num_predict Comparison\n",
    "      - Purpose: Measures the effect of num_predict (max new tokens) on output length and latency.\n",
    "      - Vars: gen_with_np(npred) times each request; prints length and lat_ms.\n",
    "      - Observation: num_predict is a ceiling, not a target; outputs can saturate before the cap.\n",
    "  - Step 18: length_hint vs num_predict\n",
    "      - Purpose: Increases length_hint to 1500 and compares num_predict=256 vs 512 (and optional\n",
    "  1024).\n",
    "      - Outcome: Shows length_ok flips to True with a higher num_predict, allowing longer drafts\n",
    "  closer to the hint.\n",
    "  - Step 19: top_p Comparison (0.80 vs 0.95)\n",
    "      - Purpose: Tests nucleus sampling breadth at fixed temperature=0.7 and seed=123.\n",
    "      - Vars: gen_with_topp(tp) prints length, scores, and a short preview.\n",
    "      - Observation: Lower top_p (0.80) tends to be more focused/longer; higher (0.95) more\n",
    "  permissive/varied and often shorter within the same num_predict.\n",
    "  - Step 20: Full Texts for top_p Runs\n",
    "      - Purpose: Prints the complete chosen text for top_p=0.80 and 0.95 so you can read and compare.\n",
    "      - Vars: g_tp080, g_tp095 hold each response. Scores printed for each.\n",
    "  - Other Parameters to Tweak (markdown)\n",
    "      - Summary: Lists knobs you can adjust:\n",
    "      - LLM: `temperature`, `top_p`, `top_k`, `num_predict`, `repeat_penalty`, `seed`, `stop`,\n",
    "  `num_ctx`, `mirostat*`.\n",
    "      - Brief: `keywords`, `goal`, `audience`, `cta`, `length_hint`, `emoji`, `link`.\n",
    "      - Variants: `num_variants` (reranking logic chooses `chosen`).\n",
    "      - Persona: tone, taboo phrases, formality, emoji/hashtags, structure.\n",
    "      - Model: `STYLGEN_EMBEDDER`, `STYLGEN_ST_MODEL`, `OLLAMA_MODEL`, logging via `STYLGEN_DEBUG`,\n",
    "  `STYLGEN_LOG_LEVEL`.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5233a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caef6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9c7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045779b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylgen-v0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
